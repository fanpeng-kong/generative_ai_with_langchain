{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 8: Customizing LLMs and Their Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-source models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='negative sentiment' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Classify the sentiment of this text: {text}\"\n",
    ")\n",
    "chain = prompt | model\n",
    "print(chain.invoke({\"text\": \"I hated that movie, it was terrible!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [{\n",
    "\"input\": \"I absolutely love the new update! Everything works seamlessly.\",\n",
    "\"output\": \"Positive\",\n",
    "},{\n",
    "\"input\": \"It's okay, but I think it could use more features.\",\n",
    "\"output\": \"Neutral\",\n",
    "}, {\n",
    "\"input\": \"I'm disappointed with the service, I expected much better performance.\",\n",
    "\"output\": \"Negative\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Positive' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"{input} -> {output}\",\n",
    "    input_variables=[\"input\", \"output\"],\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "chain = prompt | model\n",
    "print(chain.invoke({\"input\": \" This is an excellent book with high quality explanations.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='1. You start with 5 apples.\\n2. You eat 2 apples, so 5 - 2 = 3 apples.\\n3. Your friend gives you 3 apples, so 3 + 3 = 6 apples.\\n\\nSo, you now have 6 apples.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "reasoning_prompt = \"{question}\\nLet's think step by step!\"\n",
    "prompt = PromptTemplate(\n",
    "    template=reasoning_prompt,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "chain = prompt | model\n",
    "print(chain.invoke({\n",
    "    \"question\": \"There were 5 apples originally. I ate 2 apples. My friend gave me 3 apples. How many apples do I have now?\",\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [{\n",
    "    \"input\": \"I absolutely love the new update! Everything works seamlessly.\",\n",
    "    \"output\": \"Love and absolute works seamlessly are examples of positive sentiment. Therefore, the sentiment is positive\",\n",
    "    },{\n",
    "    \"input\": \"It's okay, but I think it could use more features.\",\n",
    "    \"output\": \"It's okay is not an endorsement. The customer further thinks it should be extended. Therefore, the sentiment is neutral\",\n",
    "    }, {\n",
    "    \"input\": \"I'm disappointed with the service, I expected much better performance.\",\n",
    "    \"output\": \"The customer is disappointed and expected more. This is negative\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Answer: This is a positive sentiment.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"{input} -> {output}\",\n",
    "    input_variables=[\"input\", \"output\"],\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "chain = prompt | model\n",
    "print(chain.invoke({\"input\": \" This is an excellent book with high quality explanations.\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "solutions_template = \"\"\"\n",
    "Generate {num_solutions} distinct answers to this question: {question}\n",
    "\n",
    "Solutions:\n",
    "\"\"\"\n",
    "solutions_prompt = PromptTemplate(\n",
    "    template=solutions_template,\n",
    "    input_variables=[\"question\", \"num_solutions\"]\n",
    ")\n",
    "solutions_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=solutions_prompt,\n",
    "    output_key=\"solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_template = \"\"\"\n",
    "For each answer in {solutions}, count the number of times it occurs. Finally, choose the answer that occurs most.\n",
    "\n",
    "Most frequent soution:\n",
    "\"\"\"\n",
    "consistency_prompt = PromptTemplate(\n",
    "    template=consistency_template,\n",
    "    input_variables=[\"solutions\"]\n",
    ")\n",
    "consistency_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=consistency_prompt,\n",
    "    output_key=\"best_solution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "answer_chain = SequentialChain(\n",
    "    chains=[solutions_chain, consistency_chain],\n",
    "    input_variables=[\"question\", \"num_solutions\"],\n",
    "    output_variables=[\"best_solution\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1776 occurs 5 times, so the most frequent solution is 1776.\n"
     ]
    }
   ],
   "source": [
    "print(answer_chain.run(\n",
    "    question=\"Which year was the Declaration of Independence of the United States signed?\",\n",
    "    num_solutions=\"5\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions_template = \"\"\"\n",
    "Generate {num_solutions} distinct solutions for {problem}. Consider factors like {factors}.\n",
    "\n",
    "Solutions:\n",
    "\"\"\"\n",
    "solutions_prompt = PromptTemplate(\n",
    "    template=solutions_template,\n",
    "    input_variables=[\"problem\", \"factors\", \"num_solutions\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_template = \"\"\"\n",
    "Evaluate each solution in {solutions} by analyzing pros, cons,\n",
    "feasibility, and probability of success.\n",
    "\n",
    "Evaluations:\n",
    "\"\"\"\n",
    "evaluation_prompt = PromptTemplate(\n",
    "    template=evaluation_template,\n",
    "    input_variables=[\"solutions\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_template = \"\"\"\n",
    "For the most promising solutions in {evaluations}, explain scenarios,\n",
    "implementation strategies, partnerships needed, and handling potential\n",
    "obstacles.\n",
    "\n",
    "Enhanced Reasoning:\n",
    "\"\"\"\n",
    "reasoning_prompt = PromptTemplate(\n",
    "    template=reasoning_template,\n",
    "    input_variables=[\"evaluations\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_template = \"\"\"\n",
    "Based on the evaluations and reasoning, rank the solutions in {enhanced_reasoning} from most to least promising.\n",
    "\n",
    "Ranked Solutions:\n",
    "\"\"\"\n",
    "ranking_prompt = PromptTemplate(\n",
    "    template=ranking_template,\n",
    "    input_variables=[\"enhanced_reasoning\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "solutions_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=solutions_prompt,\n",
    "    output_key=\"solutions\"\n",
    ")\n",
    "evalutation_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=evaluation_prompt,\n",
    "    output_key=\"evaluations\"\n",
    ")\n",
    "reasoning_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=reasoning_prompt,\n",
    "    output_key=\"enhanced_reasoning\"\n",
    ")\n",
    "ranking_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=ranking_prompt,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "tot_chain = SequentialChain(\n",
    "    chains=[solutions_chain, evalutation_chain, reasoning_chain, ranking_chain],\n",
    "    input_variables=[\"problem\", \"factors\", \"num_solutions\"],\n",
    "    output_variables=[\"ranked_solutions\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Implementing a comprehensive training program for engineers\n",
      "2. Developing an AI-powered tool to automate routine tasks\n",
      "3. Introducing a performance-based incentive program\n"
     ]
    }
   ],
   "source": [
    "print(tot_chain.run(\n",
    "    problem=\"Prompt engineering\",\n",
    "    factors=\"Requirements for high task performance, low token use, and few calls to the LLM\",\n",
    "    num_solutions=3\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fa8f4d2691e1735b84a26ae78635d39fd2d0e46f37776cabb5e361a9060a13d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
