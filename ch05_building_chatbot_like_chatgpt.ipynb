{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 5: Building a Chatbot Like ChatGPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from config import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding retrieval and vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.009221134094452037, 0.006869847429662192, -0.006328029137472195, -0.008716800602076323, -0.027084101053996855, 0.02807913720245218, -0.012962747434994147, -0.004613974756300044, -0.02716588420912845, -0.027111362726589063]\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text = \"This is a sample query.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(query_result[:10])\n",
    "print(len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "words = [\"cat\", \"dog\", \"computer\", \"animal\"]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "doc_vectors = embeddings.embed_documents(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "X = np.array(doc_vectors)\n",
    "dists = squareform(pdist(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_81c63_row0_col0, #T_81c63_row1_col1, #T_81c63_row2_col2, #T_81c63_row3_col3 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row0_col1 {\n",
       "  background-color: #d75445;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row0_col2 {\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row0_col3 {\n",
       "  background-color: #dc5d4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row1_col0 {\n",
       "  background-color: #d44e41;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row1_col2 {\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row1_col3 {\n",
       "  background-color: #ec7f63;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row2_col0, #T_81c63_row2_col1, #T_81c63_row2_col3, #T_81c63_row3_col2 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row3_col0 {\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_81c63_row3_col1 {\n",
       "  background-color: #e97a5f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_81c63\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_81c63_level0_col0\" class=\"col_heading level0 col0\" >cat</th>\n",
       "      <th id=\"T_81c63_level0_col1\" class=\"col_heading level0 col1\" >dog</th>\n",
       "      <th id=\"T_81c63_level0_col2\" class=\"col_heading level0 col2\" >computer</th>\n",
       "      <th id=\"T_81c63_level0_col3\" class=\"col_heading level0 col3\" >animal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_81c63_level0_row0\" class=\"row_heading level0 row0\" >cat</th>\n",
       "      <td id=\"T_81c63_row0_col0\" class=\"data row0 col0\" >0.000000</td>\n",
       "      <td id=\"T_81c63_row0_col1\" class=\"data row0 col1\" >0.523532</td>\n",
       "      <td id=\"T_81c63_row0_col2\" class=\"data row0 col2\" >0.576484</td>\n",
       "      <td id=\"T_81c63_row0_col3\" class=\"data row0 col3\" >0.522346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81c63_level0_row1\" class=\"row_heading level0 row1\" >dog</th>\n",
       "      <td id=\"T_81c63_row1_col0\" class=\"data row1 col0\" >0.523532</td>\n",
       "      <td id=\"T_81c63_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n",
       "      <td id=\"T_81c63_row1_col2\" class=\"data row1 col2\" >0.583924</td>\n",
       "      <td id=\"T_81c63_row1_col3\" class=\"data row1 col3\" >0.481125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81c63_level0_row2\" class=\"row_heading level0 row2\" >computer</th>\n",
       "      <td id=\"T_81c63_row2_col0\" class=\"data row2 col0\" >0.576484</td>\n",
       "      <td id=\"T_81c63_row2_col1\" class=\"data row2 col1\" >0.583924</td>\n",
       "      <td id=\"T_81c63_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n",
       "      <td id=\"T_81c63_row2_col3\" class=\"data row2 col3\" >0.594266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81c63_level0_row3\" class=\"row_heading level0 row3\" >animal</th>\n",
       "      <td id=\"T_81c63_row3_col0\" class=\"data row3 col0\" >0.522346</td>\n",
       "      <td id=\"T_81c63_row3_col1\" class=\"data row3 col1\" >0.481125</td>\n",
       "      <td id=\"T_81c63_row3_col2\" class=\"data row3 col2\" >0.594266</td>\n",
       "      <td id=\"T_81c63_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fcead66ace0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=dists,\n",
    "    index=words,\n",
    "    columns=words\n",
    ")\n",
    "df.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = ArxivLoader(query=\"2310.06825\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma.from_documents(documents=docs, embedding=OpenAIEmbeddings())\n",
    "# similar_vectors = vector_store.query(query_vector, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and retrieving in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "loader = WikipediaLoader(\"LangChain\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* kNN retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import KNNRetriever\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "words = [\"cat\", \"dog\", \"computer\", \"animal\"]\n",
    "retriever = KNNRetriever.from_texts(words, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='dog', metadata={}), Document(page_content='animal', metadata={}), Document(page_content='cat', metadata={}), Document(page_content='computer', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "result = retriever.get_relevant_documents(\"dog\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PubMed retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import PubMedRetriever\n",
    "retriever = PubMedRetriever()\n",
    "documents = retriever.get_relevant_documents(\"COVID\")\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effects on Quality of Life of a Telemonitoring Platform amongst Patients with Cancer (EQUALITE): A Randomized Trial Protocol.\n",
      "A Virtual Case Presentation Platform: Protocol Study.\n",
      "A Vulnerability Index to Assess the Risk of SARS-CoV-2-Related Hospitalization/Death: Urgent Need for an Update after Diffusion of Anti-COVID Vaccines.\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    print(document.metadata[\"Title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader, TextLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredEPubLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpubReader(UnstructuredEPubLoader):\n",
    "    def __init__(self, file_path: str | list[str], **kwargs: Any):\n",
    "        super().__init__(file_path, **kwargs, mode=\"elements\", strategy=\"fast\")\n",
    "        \n",
    "class DocumentLoaderException(Exception):\n",
    "    pass\n",
    "    \n",
    "class DocumentLoader(object):\n",
    "    \"\"\"Loads in a document with a supported extension.\"\"\"\n",
    "    supported_extentions = {\n",
    "        \".pdf\": PyPDFLoader,\n",
    "        \".txt\": TextLoader,\n",
    "        \".epub\": EpubReader,\n",
    "        \".docx\": UnstructuredWordDocumentLoader,\n",
    "        \".doc\": UnstructuredWordDocumentLoader\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_document(temp_filepath: str) -> list[Document]:\n",
    "    \"\"\"Load a file and return it as a list of documents.\"\"\"\n",
    "    ext = pathlib.Path(temp_filepath).suffix\n",
    "    loader = DocumentLoader.supported_extentions.get(ext)\n",
    "    if not loader:\n",
    "        raise DocumentLoaderException(\n",
    "            f\"Invalid extension type {ext}, cannot load this type of file\"\n",
    "        )\n",
    "        \n",
    "    loader = loader(temp_filepath)\n",
    "    docs = loader.load()\n",
    "    logging.info(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "def configure_retriever(\n",
    "        docs: list[Document],\n",
    "        use_compression: bool = False\n",
    "    ) -> BaseRetriever:\n",
    "    \"\"\"\"Retriever to use.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 4})\n",
    "    if not use_compression:\n",
    "        return retriever\n",
    "    \n",
    "    embeddings_filter = EmbeddingsFilter(\n",
    "        embeddings=embeddings, similarity_threshold=0.76\n",
    "    )\n",
    "    return ContextualCompressionRetriever(\n",
    "        base_compressor=embeddings_filter,\n",
    "        base_retriever=retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def configure_chain(retriever: BaseRetriever) -> Chain:\n",
    "    \"\"\"Configure chain with a retriever.\"\"\"\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True\n",
    "    )\n",
    "    return ConversationalRetrievalChain.from_llm(\n",
    "        llm, retriever=retriever, memory=memory, verbose=True, max_tokens_limit=4000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "def configure_qa_chain(uploaded_files):\n",
    "    \"\"\"Read documents, configure retriever, and the chain.\"\"\"\n",
    "    docs = []\n",
    "    temp_dir=tempfile.TemporaryDirectory()\n",
    "    for file in uploaded_files:\n",
    "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "        with open(temp_filepath, \"wb\") as f:\n",
    "            f.write(file.getvalue())\n",
    "        docs.extend(load_document(temp_filepath))\n",
    "        \n",
    "    retriever = configure_retriever(docs=docs)\n",
    "    return configure_chain(retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain.callbacks import StreamlitCallbackHandler\n",
    "\n",
    "st.set_page_config(page_title=\"LangChain: Chat with Documents\", page_icon=\"ðŸ¦œ\")\n",
    "st.title(\"ðŸ¦œ LangChain: Chat with Documents\")\n",
    "\n",
    "uploaded_files = st.sidebar.file_uploader(\n",
    "    label=\"Upload files\",\n",
    "    type=list(DocumentLoader.supported_extentions.keys()),\n",
    "    accept_multiple_files=True\n",
    ")\n",
    "if not uploaded_files:\n",
    "    st.info(\"Please upload documents to continue.\")\n",
    "    st.stop()\n",
    "    \n",
    "qa_chain = configure_qa_chain(uploaded_files)\n",
    "assistant = st.chat_message(\"assistant\")\n",
    "user_query = st.chat_input(placeholder=\"Ask me anything!\")\n",
    "\n",
    "if user_query:\n",
    "    stream_handler = StreamlitCallbackHandler(assistant)\n",
    "    response = qa_chain.run(user_query, callbacks=[stream_handler])\n",
    "    st.markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conversation buffers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you for asking. I've been busy processing a lot of information and learning new things. How can I assist you today?\n",
      "The weather today is partly cloudy with a high of 75 degrees Fahrenheit and a slight chance of rain in the afternoon. It's a great day to enjoy some outdoor activities! Is there anything else you would like to know?\n",
      "[HumanMessage(content='Hi, how are you?', additional_kwargs={}, example=False), AIMessage(content=\"Hello! I'm doing well, thank you for asking. I've been busy processing a lot of information and learning new things. How can I assist you today?\", additional_kwargs={}, example=False), HumanMessage(content=\"What's the weather like today?\", additional_kwargs={}, example=False), AIMessage(content=\"The weather today is partly cloudy with a high of 75 degrees Fahrenheit and a slight chance of rain in the afternoon. It's a great day to enjoy some outdoor activities! Is there anything else you would like to know?\", additional_kwargs={}, example=False)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True\n",
    ")\n",
    "\n",
    "chain = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "user_input = \"Hi, how are you?\"\n",
    "response = chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "user_input = \"What's the weather like today?\"\n",
    "response = chain.predict(input=user_input)\n",
    "print(response)\n",
    "\n",
    "print(memory.chat_memory.messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remembering conversation summaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human greets the AI with a simple \"hi\" and the AI responds by asking \"what\\'s up.\"'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0))\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storing knowledge graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationKGMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining several memory mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a\n",
      "human and an AI. The AI is talkative and provides lots of specific details\n",
      "from its context. If the AI does not know the answer to a question, it\n",
      "truthfully says it does not know.\n",
      "Summary of conversation:\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! How are you today?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, CombinedMemory, ConversationSummaryMemory\n",
    "\n",
    "# Initialize language model (with desired temperature parameter)\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "# Define Conversation Buffer Memory (for retaining all past messages)\n",
    "conv_memory = ConversationBufferMemory(memory_key=\"chat_history_lines\", input_key=\"input\")\n",
    "# Define Conversation Summary Memory (for summarizing conversation)\n",
    "summary_memory = ConversationSummaryMemory(llm=llm, input_key=\"input\")\n",
    "# Combine both memory types\n",
    "memory = CombinedMemory(memories=[conv_memory, summary_memory])\n",
    "# Define Prompt Template\n",
    "_DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a\n",
    "human and an AI. The AI is talkative and provides lots of specific details\n",
    "from its context. If the AI does not know the answer to a question, it\n",
    "truthfully says it does not know.\n",
    "Summary of conversation:\n",
    "{history}\n",
    "Current conversation:\n",
    "{chat_history_lines}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\", \"chat_history_lines\"], template=_DEFAULT_TEMPLATE)\n",
    "# Initialize the Conversation Chain\n",
    "conversation = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT)\n",
    "# Start the conversation\n",
    "conversation.run(\"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a\n",
      "human and an AI. The AI is talkative and provides lots of specific details\n",
      "from its context. If the AI does not know the answer to a question, it\n",
      "truthfully says it does not know.\n",
      "Summary of conversation:\n",
      "The human greets the AI with a simple \"Hi.\" The AI responds with a friendly \"Hello!\" and asks how the human is feeling today.\n",
      "Current conversation:\n",
      "Human: Hi!\n",
      "AI: Hello! How are you today?\n",
      "Human: Briefly explain to me what is Generative AI?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generative AI refers to artificial intelligence systems that are capable of creating new content, such as images, text, or music, based on patterns and data they have been trained on. These systems use algorithms to generate original content that mimics human creativity and can be used in various applications, such as art, design, and even storytelling. Generative AI has been used in fields like computer vision, natural language processing, and creative arts to produce realistic and novel outputs.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Briefly explain to me what is Generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a\n",
      "human and an AI. The AI is talkative and provides lots of specific details\n",
      "from its context. If the AI does not know the answer to a question, it\n",
      "truthfully says it does not know.\n",
      "Summary of conversation:\n",
      "The human greets the AI with a simple \"Hi.\" The AI responds with a friendly \"Hello!\" and asks how the human is feeling today. The conversation shifts to Generative AI, with the AI explaining that it refers to artificial intelligence systems that can create new content based on patterns and data they have been trained on. These systems use algorithms to generate original content that mimics human creativity and can be used in various applications. Generative AI has been utilized in fields like computer vision, natural language processing, and creative arts to produce realistic and novel outputs.\n",
      "Current conversation:\n",
      "Human: Hi!\n",
      "AI: Hello! How are you today?\n",
      "Human: Briefly explain to me what is Generative AI?\n",
      "AI: Generative AI refers to artificial intelligence systems that are capable of creating new content, such as images, text, or music, based on patterns and data they have been trained on. These systems use algorithms to generate original content that mimics human creativity and can be used in various applications, such as art, design, and even storytelling. Generative AI has been used in fields like computer vision, natural language processing, and creative arts to produce realistic and novel outputs.\n",
      "Human: How does it differ from traditional AI and Machine Learning?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Traditional AI and Machine Learning typically involve using algorithms to perform specific tasks based on predefined rules and patterns. In contrast, Generative AI goes beyond this by creating new content that is not explicitly programmed. It can generate original and creative outputs by learning from large datasets and identifying patterns to produce novel content. Generative AI is more focused on creativity and innovation, while traditional AI and Machine Learning are more task-oriented and rule-based.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"How does it differ from traditional AI and Machine Learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long-term persistence**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moderating responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import OpenAIModerationChain\n",
    "moderation_chain = OpenAIModerationChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "cot_prompt = PromptTemplate.from_template(\n",
    "    \"{question} \\nLet's think step by step!\"\n",
    ")\n",
    "llm_chain = cot_prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = llm_chain | moderation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"question\": \"What is the future of programming?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '1. Continued growth in demand: As technology becomes more integrated into everyday life, the demand for skilled programmers will continue to grow. From developing new software and applications to maintaining and updating existing systems, programmers will be essential in all industries.\\n\\n2. Specialization: As the field of programming becomes more complex, we may see a trend towards specialization. Programmers may focus on specific languages, platforms, or industries in order to develop expertise in a particular area.\\n\\n3. Automation and AI: With the rise of automation and artificial intelligence, there may be a shift towards programming machines to perform tasks that were previously done by humans. This could lead to the development of new programming languages and techniques tailored for AI and machine learning.\\n\\n4. Increased collaboration: Programming is becoming more collaborative, with teams of programmers working together on projects. This trend is likely to continue, with programmers using tools like version control systems and collaborative coding platforms to work together more efficiently.\\n\\n5. Remote work: The COVID-19 pandemic has accelerated the trend towards remote work, and programming is no exception. Many programmers now work remotely, and this trend is likely to continue in the future, with companies hiring programmers from around the world to work on projects.\\n\\n6. Continued learning: The field of programming is constantly evolving, with new languages, frameworks, and tools being developed all the time. Programmers will need to continue learning and updating their skills to stay current in the field.\\n\\nOverall, the future of programming looks bright, with continued growth in demand, increased specialization, and opportunities for collaboration and remote work. Programmers will need to adapt to new technologies and ways of working in order to stay relevant in this rapidly changing field.', 'output': '1. Continued growth in demand: As technology becomes more integrated into everyday life, the demand for skilled programmers will continue to grow. From developing new software and applications to maintaining and updating existing systems, programmers will be essential in all industries.\\n\\n2. Specialization: As the field of programming becomes more complex, we may see a trend towards specialization. Programmers may focus on specific languages, platforms, or industries in order to develop expertise in a particular area.\\n\\n3. Automation and AI: With the rise of automation and artificial intelligence, there may be a shift towards programming machines to perform tasks that were previously done by humans. This could lead to the development of new programming languages and techniques tailored for AI and machine learning.\\n\\n4. Increased collaboration: Programming is becoming more collaborative, with teams of programmers working together on projects. This trend is likely to continue, with programmers using tools like version control systems and collaborative coding platforms to work together more efficiently.\\n\\n5. Remote work: The COVID-19 pandemic has accelerated the trend towards remote work, and programming is no exception. Many programmers now work remotely, and this trend is likely to continue in the future, with companies hiring programmers from around the world to work on projects.\\n\\n6. Continued learning: The field of programming is constantly evolving, with new languages, frameworks, and tools being developed all the time. Programmers will need to continue learning and updating their skills to stay current in the field.\\n\\nOverall, the future of programming looks bright, with continued growth in demand, increased specialization, and opportunities for collaboration and remote work. Programmers will need to adapt to new technologies and ways of working in order to stay relevant in this rapidly changing field.'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fa8f4d2691e1735b84a26ae78635d39fd2d0e46f37776cabb5e361a9060a13d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
